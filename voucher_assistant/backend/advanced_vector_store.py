"""
Advanced Vector Store v·ªõi Multi-field Embedding Strategy + RAG Integration
Ph·∫ßn c·ªßa   AI Voucher Assistant - Phase 2
"""

import numpy as np
import logging
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer
from elasticsearch import Elasticsearch
import json
import re
import os
from dataclasses import dataclass
from voucher_content_generator import VoucherContentGenerator
# import openai  # Will be enabled when integrated with actual LLM service
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class RAGResponse:
    """Response t·ª´ RAG pipeline"""
    answer: str
    retrieved_vouchers: List[Dict[str, Any]]
    confidence_score: float
    search_method: str
    processing_time: float
    query_intent: Dict[str, Any]

@dataclass
class EmbeddingWeights:
    """Tr·ªçng s·ªë cho c√°c field embeddings"""
    content: float = 0.4
    location: float = 0.3
    service_type: float = 0.15
    target_audience: float = 0.1
    keywords: float = 0.05

@dataclass
class VoucherComponents:
    """C√°c component ƒë∆∞·ª£c extract t·ª´ voucher"""
    content: str
    location: str
    service_type: str
    target_audience: str
    keywords: List[str]
    price_range: str

class AdvancedVectorStore:
    """
    Advanced Vector Store v·ªõi multi-field embedding strategy
    T·ªëi ∆∞u h√≥a cho   ecosystem
    """
    
    def __init__(self, es_url: str = "http://localhost:9200", 
                 embedding_model: str = os.getenv("EMBEDDING_MODEL","dangvantuan/vietnamese-embedding"),
                 index_name: str = os.getenv('ELASTICSEARCH_INDEX', 'voucher_knowledge')):
        self.es_url = es_url
        self.es = Elasticsearch([es_url])
        self.index_name = index_name
        self.embedding_model_name = embedding_model
        self.embedding_dimension = 768
        self.content_generator = VoucherContentGenerator()
        self.weights = EmbeddingWeights()
        
        # LLM Configuration
        self.llm_model = os.getenv('LLM_MODEL', 'gpt-4o-mini')  # Fallback to OpenAI
        self.llm_api_key = os.getenv('OPENAI_API_KEY')
        self.max_context_tokens = int(os.getenv('MAX_CONTEXT_TOKENS', '4000'))
        self.temperature = float(os.getenv('LLM_TEMPERATURE', '0.3'))
        
        # Initialize embedding model
        self.model = SentenceTransformer(embedding_model)        
        logger.info(f"ü§ñ Advanced Vector Store initialized with model: {embedding_model}")
        logger.info(f"üß† LLM configured: {self.llm_model}")
        
        # Create advanced index mapping
        self._create_advanced_index()
    
    def _create_advanced_index(self):
        """T·∫°o Elasticsearch index v·ªõi advanced mapping"""
        mapping = {
            "mappings": {
                "properties": {
                    "voucher_id": {"type": "keyword"},
                    "voucher_name": {
                        "type": "text", 
                        "analyzer": "vietnamese",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "content": {
                        "type": "text", 
                        "analyzer": "vietnamese"
                    },
                    
                    # Multi-field embeddings
                    "content_embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dimension
                    },
                    "location_embedding": {
                        "type": "dense_vector", 
                        "dims": self.embedding_dimension
                    },
                    "service_embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dimension
                    },
                    "target_embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dimension
                    },
                    "combined_embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dimension
                    },
                    
                    # Structured metadata
                    "location": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "keyword"},
                            "coordinates": {"type": "geo_point"},
                            "region": {"type": "keyword"},
                            "district": {"type": "keyword"}
                        }
                    },
                    
                    "service_info": {
                        "type": "object", 
                        "properties": {
                            "category": {"type": "keyword"},
                            "subcategory": {"type": "keyword"},
                            "tags": {"type": "keyword"},
                            "has_kids_area": {"type": "boolean"},
                            "restaurant_type": {"type": "keyword"}
                        }
                    },
                    
                    "price_info": {
                        "type": "object",
                        "properties": {
                            "original_price": {"type": "long"},
                            "discounted_price": {"type": "long"},
                            "price_range": {"type": "keyword"},
                            "currency": {"type": "keyword"}
                        }
                    },
                    
                    "target_audience": {"type": "keyword"},
                    "keywords": {"type": "keyword"},
                            
                    # Additional fields for voucher details
                    "usage_instructions": {
                        "type": "text",
                        "analyzer": "vietnamese"
                    },
                    "terms_conditions": {
                        "type": "text", 
                        "analyzer": "vietnamese"
                    },
                    "merchant": {"type": "keyword"},
                    "validity_period": {"type": "keyword"},
                    
                    # Metadata
                    "metadata": {"type": "object"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            },
            "settings": {
                "analysis": {
                    "analyzer": {
                        "vietnamese": {
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop"]
                        }
                    }
                },
                "number_of_shards": 1,
                "number_of_replicas": 0
            }
        }
        
        # Create index if not exists
        if not self.es.indices.exists(index=self.index_name):
            self.es.indices.create(index=self.index_name, body=mapping)
            logger.info(f"‚úÖ Created advanced index: {self.index_name}")
    
    def extract_voucher_components(self, voucher_data: Dict[str, Any]) -> VoucherComponents:
        """
        Extract v√† classify c√°c components t·ª´ voucher data
        S·ª≠ d·ª•ng rule-based + pattern matching cho ti·∫øng Vi·ªát
        """
        # Generate content using VoucherContentGenerator if not present or needs update
        if 'content' not in voucher_data or not voucher_data['content'].strip():
            voucher_data = self.content_generator.update_voucher_with_generated_content(voucher_data)
        
        content = voucher_data.get('content', '')
        voucher_name = voucher_data.get('voucher_name', '')
        
        # Extract location
        location = self._extract_location_component(content, voucher_name)
        
        # Extract service type
        service_type = self._extract_service_type(content, voucher_name)
        
        # Extract target audience
        target_audience = self._extract_target_audience(content, voucher_name)
        
        # Extract keywords
        keywords = self._extract_keywords(content, voucher_name)
        
        # Extract price range
        price_range = self._extract_price_range(voucher_data)
        
        return VoucherComponents(
            content=content,
            location=location,
            service_type=service_type,
            target_audience=target_audience,
            keywords=keywords,
            price_range=price_range
        )
    
    def _extract_location_component(self, content: str, voucher_name: str) -> str:
        """Extract location information"""
        text = f"{voucher_name} {content}".lower()
        
        # Vietnamese location patterns
        location_patterns = [
            r'(h·∫£i ph√≤ng|hai phong)',
            r'(h√† n·ªôi|ha noi|hanoi)',
            r'(h·ªì ch√≠ minh|ho chi minh|hcm|s√†i g√≤n|saigon)',
            r'(ƒë√† n·∫µng|da nang|danang)',
            r'(c·∫ßn th∆°|can tho)',
            r'(nha trang)',
            r'(v≈©ng t√†u|vung tau)',
            r'(hu·∫ø|hue)',
            r'(ƒë√† l·∫°t|da lat)'
        ]
        
        for pattern in location_patterns:
            if re.search(pattern, text):
                match = re.search(pattern, text).group(1)
                # Normalize location names
                if 'h·∫£i ph√≤ng' in match or 'hai phong' in match:
                    return 'H·∫£i Ph√≤ng'
                elif 'h√† n·ªôi' in match or 'ha noi' in match or 'hanoi' in match:
                    return 'H√† N·ªôi'
                elif any(x in match for x in ['h·ªì ch√≠ minh', 'ho chi minh', 'hcm', 's√†i g√≤n', 'saigon']):
                    return 'H·ªì Ch√≠ Minh'
                elif 'ƒë√† n·∫µng' in match or 'da nang' in match or 'danang' in match:
                    return 'ƒê√† N·∫µng'
                # Add more normalizations as needed
        
        return 'Unknown'
    
    def _extract_service_type(self, content: str, voucher_name: str) -> str:
        """Extract service category"""
        text = f"{voucher_name} {content}".lower()
        
        service_patterns = {
            'Restaurant': [r'buffet', r'nh√† h√†ng', r'ƒÉn u·ªëng', r'qu√°n ƒÉn', r'th·ª±c ƒë∆°n', r'menu'],
            'Hotel': [r'kh√°ch s·∫°n', r'resort', r'homestay', r'villa'],
            'Entertainment': [r'gi·∫£i tr√≠', r'vui ch∆°i', r'tr√≤ ch∆°i', r'game'],
            'Shopping': [r'mua s·∫Øm', r'si√™u th·ªã', r'c·ª≠a h√†ng', r'shop'],
            'Beauty': [r'l√†m ƒë·∫πp', r'spa', r'massage', r'salon'],
            'Travel': [r'du l·ªãch', r'tour', r'v√© m√°y bay', r'kh√°ch s·∫°n'],
            'Kids': [r'tr·∫ª em', r'ƒë·ªì ch∆°i', r'kingdom', r'playground']
        }
        
        for category, patterns in service_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text):
                    return category
        
        return 'General'
    
    def _extract_target_audience(self, content: str, voucher_name: str) -> str:
        """Extract target audience"""
        text = f"{voucher_name} {content}".lower()
        
        audience_patterns = {
            'Family': [r'gia ƒë√¨nh', r'tr·∫ª em', r'family', r'kids', r'children'],
            'Couple': [r'c·∫∑p ƒë√¥i', r'couple', r'romantic', r'l√£ng m·∫°n'],
            'Business': [r'c√¥ng ty', r'doanh nghi·ªáp', r'business', r'meeting'],
            'Solo': [r'c√° nh√¢n', r'individual', r'solo'],
            'Group': [r'nh√≥m', r'group', r'team', r't·∫≠p th·ªÉ']
        }
        
        for audience, patterns in audience_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text):
                    return audience
        
        return 'General'
    
    def _extract_keywords(self, content: str, voucher_name: str) -> List[str]:
        """Extract important keywords"""
        text = f"{voucher_name} {content}".lower()
        
        # Key phrases that matter for search
        important_phrases = [
            'buffet', 'ƒÉn u·ªëng', 'tr·∫ª em', 'gia ƒë√¨nh', 'cao c·∫•p', 'sang tr·ªçng',
            'gi·∫£m gi√°', 'khuy·∫øn m√£i', 'mi·ªÖn ph√≠', 't·∫∑ng k√®m', 'combo',
            'cu·ªëi tu·∫ßn', 'l·ªÖ t·∫øt', 'ƒë·∫∑c bi·ªát', 'premium', 'luxury'
        ]
        
        found_keywords = []
        for phrase in important_phrases:
            if phrase in text:
                found_keywords.append(phrase)
        
        return found_keywords
    
    def _extract_price_range(self, voucher_data: Dict[str, Any]) -> str:
        """Classify price range"""
        try:
            price = voucher_data.get('metadata', {}).get('price', 0)
            if isinstance(price, str):
                price = float(price.replace(',', ''))
            
            if price < 100000:
                return 'Budget'
            elif price < 500000:
                return 'Mid-range'
            elif price < 1000000:
                return 'Premium'
            else:
                return 'Luxury'
        except:
            return 'Unknown'
    
    def create_multi_field_embeddings(self, components: VoucherComponents) -> Dict[str, np.ndarray]:
        """
        T·∫°o embeddings ri√™ng bi·ªát cho t·ª´ng field
        """
        embeddings = {}
        
        # Content embedding (full text)
        embeddings['content'] = self.model.encode(components.content)
        
        # Location embedding (focused on location)
        location_text = f"ƒê·ªãa ƒëi·ªÉm: {components.location}. Khu v·ª±c: {components.location}"
        embeddings['location'] = self.model.encode(location_text)
        
        # Service embedding (focused on service type and features)
        service_text = f"D·ªãch v·ª•: {components.service_type}. Keywords: {', '.join(components.keywords)}"
        embeddings['service'] = self.model.encode(service_text)
        
        # Target audience embedding
        target_text = f"ƒê·ªëi t∆∞·ª£ng: {components.target_audience}. Ph√π h·ª£p cho: {components.target_audience}"
        embeddings['target'] = self.model.encode(target_text)
        
        return embeddings
    
    def combine_embeddings(self, embeddings: Dict[str, np.ndarray]) -> np.ndarray:
        """
        K·∫øt h·ª£p c√°c embeddings v·ªõi tr·ªçng s·ªë
        """
        combined = (
            embeddings['content'] * self.weights.content +
            embeddings['location'] * self.weights.location +
            embeddings['service'] * self.weights.service_type +
            embeddings['target'] * self.weights.target_audience
        )
        
        # Normalize the combined embedding
        combined = combined / np.linalg.norm(combined)
        
        return combined
    
    async def index_voucher_advanced(self, voucher_data: Dict[str, Any]) -> bool:
        """
        Index voucher v·ªõi advanced multi-field strategy
        """
        try:
            # Extract components
            components = self.extract_voucher_components(voucher_data)
            
            # Create multi-field embeddings
            embeddings = self.create_multi_field_embeddings(components)
            
            # Combine embeddings
            combined_embedding = self.combine_embeddings(embeddings)
            
            # Prepare document for indexing
            doc = {
                'voucher_id': voucher_data.get('voucher_id'),
                'voucher_name': voucher_data.get('voucher_name'),
                'content': components.content,
                
                # Multi-field embeddings
                'content_embedding': embeddings['content'].tolist(),
                'location_embedding': embeddings['location'].tolist(),
                'service_embedding': embeddings['service'].tolist(),
                'target_embedding': embeddings['target'].tolist(),
                'combined_embedding': combined_embedding.tolist(),
                
                # Structured metadata
                'location': {
                    'name': components.location,
                    'region': self._get_region(components.location),
                    'district': voucher_data.get('metadata', {}).get('district', '')
                },
                
                'service_info': {
                    'category': components.service_type,
                    'tags': components.keywords,
                    'has_kids_area': 'tr·∫ª em' in components.keywords,
                    'restaurant_type': 'buffet' if 'buffet' in components.keywords else 'other'
                },
                
                'price_info': {
                    'original_price': voucher_data.get('metadata', {}).get('price', 0),
                    'price_range': components.price_range,
                    'currency': 'VND'
                },
                
                'target_audience': components.target_audience,
                'keywords': components.keywords,
                'created_at': voucher_data.get('created_at'),
                'updated_at': voucher_data.get('updated_at', voucher_data.get('created_at'))
            }
            
            # Index document
            response = self.es.index(
                index=self.index_name,
                id=voucher_data.get('voucher_id'),
                body=doc
            )
            
            logger.info(f"‚úÖ Indexed voucher {voucher_data.get('voucher_id')} with advanced embeddings")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error indexing voucher: {e}")
            return False
    
    def _get_region(self, location: str) -> str:
        """Map location to region"""
        region_mapping = {
            'H√† N·ªôi': 'Mi·ªÅn B·∫Øc',
            'H·∫£i Ph√≤ng': 'Mi·ªÅn B·∫Øc', 
            'ƒê√† N·∫µng': 'Mi·ªÅn Trung',
            'H·ªì Ch√≠ Minh': 'Mi·ªÅn Nam',
            'C·∫ßn Th∆°': 'Mi·ªÅn Nam'
        }
        return region_mapping.get(location, 'Unknown')
    
    async def advanced_vector_search(self, query: str, top_k: int = 10,
                                   location_filter: Optional[str] = None,
                                   service_filter: Optional[str] = None,
                                   price_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Advanced search v·ªõi multi-field embedding v√† filtering
        """
        try:
            # Extract query components
            query_components = self._analyze_query(query)
            
            # Create query embedding based on detected intent
            query_embedding = self._create_adaptive_query_embedding(query, query_components)
            
            # Build Elasticsearch query
            search_body = self._build_advanced_search_query(
                query_embedding, query_components, top_k, 
                location_filter, service_filter, price_filter
            )
            
            # üîç Log Elasticsearch query for debugging
            logger.info(f"üîç Elasticsearch Query for '{query}':")
            logger.info(f"üìã Query Body: {json.dumps(search_body, indent=2, ensure_ascii=False)}")
            
            # Execute search
            response = self.es.search(index=self.index_name, body=search_body)
            
            # Process and rank results
            results = self._process_advanced_results(response, query_components)
            
            logger.info(f"‚úÖ Advanced search completed: {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Advanced search error: {e}")
            return []
    
    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch query ƒë·ªÉ hi·ªÉu intent v√† components"""
        query_lower = query.lower()
        
        components = {
            'original_query': query,  # üî• Add original query
            'location_intent': None,
            'service_intent': None,
            'target_intent': None,
            'primary_focus': 'content'  # default
        }
        
        # Detect location intent
        location_keywords = ['t·∫°i', '·ªü', 'trong', 'g·∫ßn']
        for keyword in location_keywords:
            if keyword in query_lower:
                components['location_intent'] = 'high'
                components['primary_focus'] = 'location'
                break
        
        # Detect service intent
        service_keywords = ['buffet', 'nh√† h√†ng', 'ƒÉn', 'u·ªëng', 'spa', 'massage']
        for keyword in service_keywords:
            if keyword in query_lower:
                components['service_intent'] = 'high'
                break
        
        # Detect target intent
        target_keywords = ['tr·∫ª em', 'gia ƒë√¨nh', 'family', 'kids']
        for keyword in target_keywords:
            if keyword in query_lower:
                components['target_intent'] = 'high'
                break
        
        return components
    
    def _create_adaptive_query_embedding(self, query: str, components: Dict[str, Any]) -> np.ndarray:
        """
        T·∫°o query embedding th√≠ch ·ª©ng d·ª±a tr√™n intent
        Tr·∫£ v·ªÅ base embedding ƒë·ªÉ d√πng chung cho t·∫•t c·∫£ fields
        """
        # Create enhanced query text based on detected intent
        enhanced_query = query
        
        if components['location_intent'] == 'high':
            # Enhance with location context
            enhanced_query = f"ƒê·ªãa ƒëi·ªÉm ƒë·ªãa l√Ω khu v·ª±c: {query}"
        elif components['service_intent'] == 'high':
            # Enhance with service context
            enhanced_query = f"D·ªãch v·ª• lo·∫°i h√¨nh: {query}"
        elif components['target_intent'] == 'high':
            # Enhance with target context
            enhanced_query = f"ƒê·ªëi t∆∞·ª£ng ph√π h·ª£p: {query}"
        
        # Create base embedding for the enhanced query
        base_embedding = self.model.encode(enhanced_query)
        
        return base_embedding
    
    def _build_advanced_search_query(self, query_embedding: np.ndarray, 
                                   query_components: Dict[str, Any],
                                   top_k: int,
                                   location_filter: Optional[str] = None,
                                   service_filter: Optional[str] = None,
                                   price_filter: Optional[str] = None) -> Dict[str, Any]:
        """Build sophisticated Elasticsearch query with MULTI-FIELD VECTOR SEARCH"""
        
        # üéØ Dynamic weights based on query intent
        weights = self._calculate_dynamic_weights(query_components)
        
        # üöÄ MULTI-FIELD VECTOR SEARCH: Search all embedding fields simultaneously
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        # üéØ Exact text search (high boost for brand names)
                        {
                            "multi_match": {
                                "query": query_components.get('original_query', ''),
                                "fields": ["voucher_name^3", "content^1"],
                                "type": "best_fields",
                                "boost": 2.0  # Text search boost
                            }
                        },
                        # ü§ñ Multi-field semantic search with adaptive weights
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": """
                                        double contentScore = cosineSimilarity(params.query_vector, 'content_embedding') * params.content_weight;
                                        double locationScore = cosineSimilarity(params.query_vector, 'location_embedding') * params.location_weight;
                                        double serviceScore = cosineSimilarity(params.query_vector, 'service_embedding') * params.service_weight;
                                        double targetScore = cosineSimilarity(params.query_vector, 'target_embedding') * params.target_weight;
                                        double combinedScore = cosineSimilarity(params.query_vector, 'combined_embedding') * params.combined_weight;
                                        
                                        return (contentScore + locationScore + serviceScore + targetScore + combinedScore) + 1.0;
                                    """,
                                    "params": {
                                        "query_vector": query_embedding.tolist(),
                                        "content_weight": weights['content'],
                                        "location_weight": weights['location'],
                                        "service_weight": weights['service'],
                                        "target_weight": weights['target'],
                                        "combined_weight": weights['combined']
                                    }
                                },
                                "boost": 3.0  # High boost for semantic similarity
                            }
                        }
                    ],
                    "filter": []
                }
            },
            "size": top_k,
            "_source": ["voucher_id", "voucher_name", "content", "location", "service_info", "price_info", "target_audience"]
        }
        
        # Add filters
        if location_filter:
            search_body["query"]["bool"]["filter"].append({
                "term": {"location.name": location_filter}
            })
        
        if service_filter:
            search_body["query"]["bool"]["filter"].append({
                "term": {"service_info.category": service_filter}
            })
        
        if price_filter:
            search_body["query"]["bool"]["filter"].append({
                "term": {"price_info.price_range": price_filter}
            })
        
        return search_body
    
    def _calculate_dynamic_weights(self, query_components: Dict[str, Any]) -> Dict[str, float]:
        """Calculate adaptive weights based on query intent"""
        # Base weights
        weights = {
            'content': 0.3,
            'location': 0.2,
            'service': 0.2,
            'target': 0.1,
            'combined': 0.2  # Always maintain some combined score
        }
        
        # Adjust weights based on detected intent
        if query_components.get('location_intent') == 'high':
            weights['location'] = 0.4
            weights['content'] = 0.2
            weights['combined'] = 0.3
            weights['service'] = 0.05
            weights['target'] = 0.05
            
        elif query_components.get('service_intent') == 'high':
            weights['service'] = 0.4
            weights['content'] = 0.2
            weights['combined'] = 0.3
            weights['location'] = 0.05
            weights['target'] = 0.05
            
        elif query_components.get('target_intent') == 'high':
            weights['target'] = 0.4
            weights['content'] = 0.2
            weights['combined'] = 0.3
            weights['location'] = 0.05
            weights['service'] = 0.05
        
        return weights
    
    def _process_advanced_results(self, response: Dict[str, Any], 
                                query_components: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process and enhance search results"""
        results = []
        
        for hit in response.get('hits', {}).get('hits', []):
            score = hit['_score'] - 1.0  # Normalize
            source = hit['_source']
            
            # Apply additional boosting based on query components
            if query_components.get('location_intent') == 'high':
                # Additional location relevance boost already handled by embedding choice
                pass
            
            result = {
                'voucher_id': source.get('voucher_id'),
                'voucher_name': source.get('voucher_name'),
                'content': source.get('content'),
                'similarity_score': round(score, 4),
                'location': source.get('location', {}),
                'service_info': source.get('service_info', {}),
                'price_info': source.get('price_info', {}),
                'target_audience': source.get('target_audience'),
                'search_method': 'advanced_multi_field'
            }
            
            results.append(result)
        
        return results
    
    # ================== RAG INTEGRATION METHODS ==================
    
    async def rag_search_with_llm(self, query: str, top_k: int = 5,
                                 location_filter: Optional[str] = None,
                                 service_filter: Optional[str] = None,
                                 price_filter: Optional[str] = None) -> RAGResponse:
        """
        Complete RAG pipeline: Retrieve + Generate
        """
        start_time = datetime.now()
        
        try:
            # 1. Retrieve relevant vouchers using advanced search
            logger.info(f"üîç RAG Pipeline started for query: '{query}'")
            retrieved_vouchers = await self.advanced_vector_search(
                query, top_k=top_k,
                location_filter=location_filter,
                service_filter=service_filter, 
                price_filter=price_filter
            )
            
            # 2. Extract query components for context
            query_components = self._analyze_query(query)
            
            # 3. Prepare context for LLM
            context = self._prepare_llm_context(retrieved_vouchers, query_components)
            
            # 4. Generate answer using LLM
            if not retrieved_vouchers:
                answer = self._generate_no_results_response(query)
                confidence_score = 0.0
            else:
                answer = await self._call_llm_with_context(query, context, query_components)
                confidence_score = self._calculate_confidence_score(retrieved_vouchers)
            
            # 5. Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"‚úÖ RAG completed in {processing_time:.2f}s, confidence: {confidence_score:.2f}")
            
            return RAGResponse(
                answer=answer,
                retrieved_vouchers=retrieved_vouchers,
                confidence_score=confidence_score,
                search_method='advanced_rag',
                processing_time=processing_time,
                query_intent=query_components
            )
            
        except Exception as e:
            logger.error(f"‚ùå RAG pipeline error: {e}")
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return RAGResponse(
                answer="Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
                retrieved_vouchers=[],
                confidence_score=0.0,
                search_method='error',
                processing_time=processing_time,
                query_intent={}
            )
    
    def _prepare_llm_context(self, retrieved_vouchers: List[Dict[str, Any]], 
                           query_components: Dict[str, Any]) -> str:
        """
        Chu·∫©n b·ªã context t·ª´ retrieved vouchers cho LLM
        """
        if not retrieved_vouchers:
            return "Kh√¥ng t√¨m th·∫•y voucher ph√π h·ª£p."
        
        context_parts = []
        context_parts.append("=== TH√îNG TIN VOUCHER LI√äN QUAN ===\n")
        
        for i, voucher in enumerate(retrieved_vouchers, 1):
            context_parts.append(f"VOUCHER {i}:")
            context_parts.append(f"T√™n: {voucher.get('voucher_name', 'N/A')}")
            context_parts.append(f"N·ªôi dung: {voucher.get('content', 'N/A')}")
            
            # Add structured metadata
            location = voucher.get('location', {})
            if location.get('name') != 'Unknown':
                context_parts.append(f"ƒê·ªãa ƒëi·ªÉm: {location.get('name')} ({location.get('region', '')})")
            
            service_info = voucher.get('service_info', {})
            if service_info.get('category'):
                context_parts.append(f"Lo·∫°i d·ªãch v·ª•: {service_info.get('category')}")
            
            price_info = voucher.get('price_info', {})
            if price_info.get('price_range'):
                context_parts.append(f"Ph√¢n kh√∫c gi√°: {price_info.get('price_range')}")
            
            target_audience = voucher.get('target_audience')
            if target_audience and target_audience != 'General':
                context_parts.append(f"Ph√π h·ª£p cho: {target_audience}")
            
            similarity_score = voucher.get('similarity_score', 0)
            context_parts.append(f"ƒê·ªô ph√π h·ª£p: {similarity_score:.2f}")
            context_parts.append("---")
        
        # Limit context length
        full_context = "\n".join(context_parts)
        if len(full_context) > self.max_context_tokens * 3:  # Rough token estimation
            # Truncate to most relevant vouchers
            truncated_vouchers = retrieved_vouchers[:3]
            return self._prepare_llm_context(truncated_vouchers, query_components)
        
        return full_context
    
    async def _call_llm_with_context(self, query: str, context: str, 
                                   query_components: Dict[str, Any]) -> str:
        """
        G·ªçi LLM v·ªõi context ƒë·ªÉ generate answer
        """
        try:
            # Determine response style based on query intent
            response_style = self._get_response_style(query_components)
            
            system_prompt = f"""B·∫°n l√† AI Assistant chuy√™n v·ªÅ voucher   - h·ªá sinh th√°i FnB h√†ng ƒë·∫ßu Vi·ªát Nam.

NHI·ªÜM V·ª§:
- Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin voucher ƒë∆∞·ª£c cung c·∫•p
- ƒê∆∞a ra l·ªùi khuy√™n ph√π h·ª£p v√† chi ti·∫øt
- Gi·∫£i th√≠ch c√°c ƒëi·ªÅu kho·∫£n & ƒëi·ªÅu ki·ªán m·ªôt c√°ch d·ªÖ hi·ªÉu
- G·ª£i √Ω voucher ph√π h·ª£p nh·∫•t

PHONG C√ÅCH TR·∫¢ L·ªúI: {response_style}

QUY T·∫ÆC:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ voucher ƒë∆∞·ª£c cung c·∫•p
2. KH√îNG t·ª± t·∫°o ra th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu
3. N·∫øu kh√¥ng c√≥ voucher ph√π h·ª£p, gi·∫£i th√≠ch l√Ω do v√† g·ª£i √Ω t√¨m ki·∫øm kh√°c
4. Lu√¥n k·∫øt th√∫c v·ªõi c√¢u h·ªèi ƒë·ªÉ t∆∞∆°ng t√°c th√™m
5. S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ l√†m cho c√¢u tr·∫£ l·ªùi sinh ƒë·ªông

TH√îNG TIN VOUCHER:
{context}"""

            user_prompt = f"C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {query}"
            
            # Call LLM (using simple HTTP request to avoid openai dependency for now)
            response = await self._make_llm_request(system_prompt, user_prompt)
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå LLM call failed: {e}")
            return self._generate_fallback_response(query, context)
    
    def _get_response_style(self, query_components: Dict[str, Any]) -> str:
        """Determine appropriate response style based on query intent"""
        if query_components.get('location_intent') == 'high':
            return "T·∫≠p trung v√†o th√¥ng tin ƒë·ªãa ƒëi·ªÉm, khu v·ª±c v√† h∆∞·ªõng d·∫´n ƒë∆∞·ªùng ƒëi"
        elif query_components.get('service_intent') == 'high':
            return "Chi ti·∫øt v·ªÅ d·ªãch v·ª•, ti·ªán √≠ch v√† tr·∫£i nghi·ªám"
        elif query_components.get('target_intent') == 'high':
            return "T∆∞ v·∫•n ph√π h·ª£p v·ªõi ƒë·ªëi t∆∞·ª£ng v√† nhu c·∫ßu c·ª• th·ªÉ"
        else:
            return "T·ªïng quan v√† g·ª£i √Ω to√†n di·ªán"
    
    async def _make_llm_request(self, system_prompt: str, user_prompt: str) -> str:
        """
        Make LLM request (simplified version without openai dependency)
        In production, integrate with Vertex AI or OpenAI
        """
        # For now, return a structured response based on context
        return self._generate_structured_response(user_prompt, system_prompt)
    
    def _generate_structured_response(self, query: str, context: str) -> str:
        """
        Generate structured response when LLM is not available
        """
        # Extract voucher count from context
        voucher_count = context.count("VOUCHER ")
        
        if voucher_count == 0:
            return f"""üîç T√¥i ƒë√£ t√¨m ki·∫øm cho "{query}" nh∆∞ng kh√¥ng t√¨m th·∫•y voucher ph√π h·ª£p.

üí° **G·ª£i √Ω:**
- Th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a kh√°c
- Ki·ªÉm tra l·∫°i ƒë·ªãa ƒëi·ªÉm ho·∫∑c lo·∫°i d·ªãch v·ª•
- Li√™n h·ªá hotline   ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ th√™m

‚ùì B·∫°n c√≥ mu·ªën t√¥i t√¨m ki·∫øm voucher theo ti√™u ch√≠ kh√°c kh√¥ng?"""
        
        response_parts = [
            f"üéØ T√¥i t√¨m th·∫•y **{voucher_count} voucher** ph√π h·ª£p v·ªõi y√™u c·∫ßu \"{query}\" c·ªßa b·∫°n:\n"
        ]
        
        # Extract voucher names from context
        voucher_lines = [line for line in context.split('\n') if line.startswith('T√™n:')]
        for i, line in enumerate(voucher_lines[:3], 1):
            voucher_name = line.replace('T√™n: ', '')
            response_parts.append(f"**{i}.** {voucher_name}")
        
        response_parts.extend([
            "",
            "üí° **L·ªùi khuy√™n:**",
            "- Ki·ªÉm tra ƒëi·ªÅu kho·∫£n s·ª≠ d·ª•ng tr∆∞·ªõc khi ƒë·∫∑t",
            "- ƒê·∫∑t b√†n tr∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ch·ªó",
            "- Mang theo voucher khi ƒë·∫øn s·ª≠ d·ª•ng",
            "",
            "‚ùì B·∫°n c√≥ mu·ªën t√¥i gi·∫£i th√≠ch chi ti·∫øt v·ªÅ voucher n√†o kh√¥ng?"
        ])
        
        return "\n".join(response_parts)
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """Generate fallback response when LLM fails"""
        return f"""‚ö° D·ª±a tr√™n t√¨m ki·∫øm cho "{query}", t√¥i t√¨m th·∫•y m·ªôt s·ªë voucher c√≥ th·ªÉ ph√π h·ª£p:

{context[:500]}...

üíº ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n, vui l√≤ng li√™n h·ªá hotline   ho·∫∑c th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a c·ª• th·ªÉ h∆°n.

‚ùì B·∫°n c√≥ c√¢u h·ªèi g√¨ kh√°c v·ªÅ voucher kh√¥ng?"""
    
    def _generate_no_results_response(self, query: str) -> str:
        """Generate response when no vouchers found"""
        return f"""üîç Kh√¥ng t√¨m th·∫•y voucher ph√π h·ª£p v·ªõi "{query}".

üí° **Th·ª≠ c√°c c√°ch sau:**
- T√¨m ki·∫øm v·ªõi t·ª´ kh√≥a ƒë∆°n gi·∫£n h∆°n (VD: "buffet", "massage", "spa")
- Ch·ªâ ƒë·ªãnh ƒë·ªãa ƒëi·ªÉm c·ª• th·ªÉ (VD: "H√† N·ªôi", "TP.HCM")
- T√¨m theo lo·∫°i d·ªãch v·ª• (VD: "nh√† h√†ng", "kh√°ch s·∫°n")

üåü **G·ª£i √Ω ph·ªï bi·∫øn:**
- "buffet H·∫£i Ph√≤ng" 
- "spa cho gia ƒë√¨nh"
- "nh√† h√†ng cao c·∫•p"

‚ùì B·∫°n c√≥ mu·ªën th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a kh√°c kh√¥ng?"""
    
    def _calculate_confidence_score(self, retrieved_vouchers: List[Dict[str, Any]]) -> float:
        """Calculate confidence score based on retrieval results"""
        if not retrieved_vouchers:
            return 0.0
        
        # Calculate based on similarity scores and result count
        avg_similarity = sum(v.get('similarity_score', 0) for v in retrieved_vouchers) / len(retrieved_vouchers)
        
        # Normalize to 0-1 range
        confidence = min(avg_similarity / 50.0, 1.0)  # Assuming max similarity ~50
        
        # Boost confidence if we have multiple good results
        if len(retrieved_vouchers) >= 3 and avg_similarity > 30:
            confidence = min(confidence * 1.2, 1.0)
        
        return round(confidence, 3)
    
    # ================== UNIFIED SEARCH INTERFACE ==================
    
    async def search(self, query: str, 
                    search_type: str = "rag",  # "rag", "vector", "hybrid"
                    top_k: int = 5,
                    location_filter: Optional[str] = None,
                    service_filter: Optional[str] = None,
                    price_filter: Optional[str] = None,
                    return_raw_results: bool = False) -> Dict[str, Any]:
        """
        Unified search interface supporting multiple search types
        
        Args:
            query: Search query in Vietnamese
            search_type: "rag" (full RAG pipeline), "vector" (vector search only), "hybrid"
            top_k: Number of results to return
            location_filter: Filter by specific location
            service_filter: Filter by service category  
            price_filter: Filter by price range
            return_raw_results: If True, return raw search results instead of RAG response
            
        Returns:
            RAGResponse for "rag" search_type, or raw results for others
        """
        if search_type == "rag":
            # Use full RAG pipeline (Retrieval + Generation)
            return await self.rag_search_with_llm(
                query=query,
                top_k=top_k,
                location_filter=location_filter,
                service_filter=service_filter,
                price_filter=price_filter
            )
        
        elif search_type == "vector":
            # Use advanced vector search only
            results = await self.advanced_vector_search(
                query=query,
                top_k=top_k,
                location_filter=location_filter,
                service_filter=service_filter,
                price_filter=price_filter
            )
            
            if return_raw_results:
                return {"results": results, "search_type": "vector"}
            else:
                # Wrap in RAGResponse format for consistency
                return RAGResponse(
                    answer=f"T√¨m th·∫•y {len(results)} voucher ph√π h·ª£p. ƒê√¢y l√† k·∫øt qu·∫£ vector search thu·∫ßn t√∫y.",
                    retrieved_vouchers=results,
                    confidence_score=self._calculate_confidence_score(results),
                    search_method='vector_only',
                    processing_time=0.0,
                    query_intent=self._analyze_query(query)
                )
        
        elif search_type == "hybrid":
            # Hybrid approach: Vector search + minimal context enhancement
            results = await self.advanced_vector_search(
                query=query,
                top_k=top_k,
                location_filter=location_filter,
                service_filter=service_filter,
                price_filter=price_filter
            )
            
            # Generate minimal response without full LLM call
            if results:
                answer = self._generate_hybrid_response(query, results)
            else:
                answer = self._generate_no_results_response(query)
            
            return RAGResponse(
                answer=answer,
                retrieved_vouchers=results,
                confidence_score=self._calculate_confidence_score(results),
                search_method='hybrid',
                processing_time=0.0,
                query_intent=self._analyze_query(query)
            )
        
        else:
            raise ValueError(f"Unsupported search_type: {search_type}. Use 'rag', 'vector', or 'hybrid'")
    
    def _generate_hybrid_response(self, query: str, results: List[Dict[str, Any]]) -> str:
        """Generate a hybrid response with voucher list and basic guidance"""
        if not results:
            return self._generate_no_results_response(query)
        
        response_parts = [
            f"üéØ **K·∫øt qu·∫£ t√¨m ki·∫øm cho**: \"{query}\"",
            f"üìä **T√¨m th·∫•y**: {len(results)} voucher ph√π h·ª£p\n"
        ]
        
        # List top vouchers with key details
        for i, voucher in enumerate(results[:3], 1):
            name = voucher.get('voucher_name', 'N/A')
            location = voucher.get('location', {}).get('name', 'N/A')
            similarity = voucher.get('similarity_score', 0)
            
            response_parts.append(f"**{i}. {name}**")
            if location != 'N/A':
                response_parts.append(f"   üìç {location}")
            response_parts.append(f"   ‚≠ê ƒê·ªô ph√π h·ª£p: {similarity:.1f}%\n")
        
        if len(results) > 3:
            response_parts.append(f"... v√† {len(results) - 3} voucher kh√°c")
        
        response_parts.extend([
            "\nüí° **ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n, h√£y s·ª≠ d·ª•ng ch·∫ø ƒë·ªô RAG search!**",
            "‚ùì B·∫°n c√≥ mu·ªën bi·∫øt th√™m th√¥ng tin v·ªÅ voucher n√†o kh√¥ng?"
        ])
        
        return "\n".join(response_parts)
