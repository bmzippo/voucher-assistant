"""
Smart Query Parser cho   AI Voucher Assistant
Ph√¢n t√≠ch v√† hi·ªÉu √Ω ƒë·ªãnh ng∆∞·ªùi d√πng t·ª´ natural language queries
"""

import re
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    """C√°c lo·∫°i intent ch√≠nh"""
    FIND_RESTAURANT = "find_restaurant"
    FIND_HOTEL = "find_hotel" 
    FIND_ENTERTAINMENT = "find_entertainment"
    FIND_SHOPPING = "find_shopping"
    FIND_BEAUTY = "find_beauty"
    FIND_TRAVEL = "find_travel"
    FIND_KIDS = "find_kids"
    GENERAL_SEARCH = "general_search"

class LocationType(Enum):
    """Lo·∫°i ƒë·ªãa ƒëi·ªÉm"""
    CITY = "city"
    DISTRICT = "district"  
    LANDMARK = "landmark"
    REGION = "region"

@dataclass 
class QueryComponents:
    """C√°c th√†nh ph·∫ßn ƒë∆∞·ª£c extract t·ª´ query"""
    original_query: str
    intent: QueryIntent
    location: Optional[str] = None
    location_type: Optional[LocationType] = None
    service_requirements: List[str] = None
    target_audience: Optional[str] = None
    price_preference: Optional[str] = None
    time_requirements: List[str] = None
    keywords: List[str] = None
    modifiers: List[str] = None
    confidence: float = 0.0

class SmartQueryParser:
    """
    Advanced Query Parser cho Vietnamese natural language
    Hi·ªÉu v√† ph√¢n t√≠ch √Ω ƒë·ªãnh ng∆∞·ªùi d√πng
    """
    
    def __init__(self):
        self.intent_patterns = self._load_intent_patterns()
        self.location_patterns = self._load_location_patterns()
        self.service_patterns = self._load_service_patterns()
        self.target_patterns = self._load_target_patterns()
        self.time_patterns = self._load_time_patterns()
        self.modifier_patterns = self._load_modifier_patterns()
        
        logger.info("üß† Smart Query Parser initialized")
    
    def _load_intent_patterns(self) -> Dict[QueryIntent, List[str]]:
        """Load patterns for intent detection - support both with and without diacritics"""
        return {
            QueryIntent.FIND_RESTAURANT: [
                r'(qu√°n ƒÉn|quan an|nh√† h√†ng|nha hang|ƒÉn u·ªëng|an uong|buffet|th·ª©c ƒÉn|thuc an|m√≥n ƒÉn|mon an|b·ªØa ƒÉn|bua an)',
                r'(restaurant|food|eat|dining|meal|cafe|quan cafe)',
                r'(ƒë√≥i|doi|th√®m|them|mu·ªën ƒÉn|muon an)',
                r'(bellissimo|silk path|sheraton|renaissance|capella|mercure|daewoo)'  # Brand names
            ],
            QueryIntent.FIND_HOTEL: [
                r'(kh√°ch s·∫°n|khach san|resort|homestay|villa|n∆°i ·ªü|noi o|ngh·ªâ d∆∞·ª°ng|nghi duong)',
                r'(hotel|accommodation|stay|lodge)',
                r'(ng·ªß|ngu|ngh·ªâ|nghi|·ªü l·∫°i|o lai)'
            ],
            QueryIntent.FIND_ENTERTAINMENT: [
                r'(gi·∫£i tr√≠|giai tri|vui ch∆°i|vui choi|tr√≤ ch∆°i|tro choi|game|s·ª± ki·ªán|su kien|show)',
                r'(entertainment|fun|play|event|activity)',
                r'(ch∆°i|choi|vui|th∆∞ gi√£n|thu gian)'
            ],
            QueryIntent.FIND_SHOPPING: [
                r'(mua s·∫Øm|mua sam|c·ª≠a h√†ng|cua hang|si√™u th·ªã|sieu thi|mall|shopping)',
                r'(shop|store|buy|purchase)',
                r'(mua|s·∫Øm|sam|t√¨m mua|tim mua)'
            ],
            QueryIntent.FIND_BEAUTY: [
                r'(l√†m ƒë·∫πp|lam dep|spa|massage|salon|nail|t√≥c|toc)',
                r'(beauty|wellness|relaxation)',
                r'(ƒë·∫πp|dep|th∆∞ gi√£n|thu gian|chƒÉm s√≥c|cham soc)'
            ],
            QueryIntent.FIND_KIDS: [
                r'(tr·∫ª em|tre em|tr·∫ª con|tre con|b√© y√™u|be yeu|em b√©|em be|children|kids)',
                r'(ƒë·ªì ch∆°i|do choi|playground|khu vui ch∆°i|khu vui choi)',
                r'(family|gia ƒë√¨nh.*tr·∫ª|gia dinh.*tre)'
            ]
        }
    
    def _load_location_patterns(self) -> Dict[str, Dict[str, Any]]:
        """Load location patterns with metadata - support both with and without diacritics"""
        return {
            'cities': {
                # With diacritics
                'h·∫£i ph√≤ng': {'normalized': 'H·∫£i Ph√≤ng', 'region': 'Mi·ªÅn B·∫Øc', 'type': LocationType.CITY},
                'h√† n·ªôi': {'normalized': 'H√† N·ªôi', 'region': 'Mi·ªÅn B·∫Øc', 'type': LocationType.CITY},
                'h·ªì ch√≠ minh': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'hcm': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                's√†i g√≤n': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'ƒë√† n·∫µng': {'normalized': 'ƒê√† N·∫µng', 'region': 'Mi·ªÅn Trung', 'type': LocationType.CITY},
                'c·∫ßn th∆°': {'normalized': 'C·∫ßn Th∆°', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'nha trang': {'normalized': 'Nha Trang', 'region': 'Mi·ªÅn Trung', 'type': LocationType.CITY},
                'v≈©ng t√†u': {'normalized': 'V≈©ng T√†u', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'hu·∫ø': {'normalized': 'Hu·∫ø', 'region': 'Mi·ªÅn Trung', 'type': LocationType.CITY},
                'ƒë√† l·∫°t': {'normalized': 'ƒê√† L·∫°t', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                # Without diacritics
                'hai phong': {'normalized': 'H·∫£i Ph√≤ng', 'region': 'Mi·ªÅn B·∫Øc', 'type': LocationType.CITY},
                'ha noi': {'normalized': 'H√† N·ªôi', 'region': 'Mi·ªÅn B·∫Øc', 'type': LocationType.CITY},
                'hanoi': {'normalized': 'H√† N·ªôi', 'region': 'Mi·ªÅn B·∫Øc', 'type': LocationType.CITY},
                'ho chi minh': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'saigon': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'sai gon': {'normalized': 'H·ªì Ch√≠ Minh', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'da nang': {'normalized': 'ƒê√† N·∫µng', 'region': 'Mi·ªÅn Trung', 'type': LocationType.CITY},
                'can tho': {'normalized': 'C·∫ßn Th∆°', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'vung tau': {'normalized': 'V≈©ng T√†u', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY},
                'hue': {'normalized': 'Hu·∫ø', 'region': 'Mi·ªÅn Trung', 'type': LocationType.CITY},
                'da lat': {'normalized': 'ƒê√† L·∫°t', 'region': 'Mi·ªÅn Nam', 'type': LocationType.CITY}
            },
            'districts': {
                'qu·∫≠n 1': {'normalized': 'Qu·∫≠n 1', 'city': 'H·ªì Ch√≠ Minh', 'type': LocationType.DISTRICT},
                'qu·∫≠n 3': {'normalized': 'Qu·∫≠n 3', 'city': 'H·ªì Ch√≠ Minh', 'type': LocationType.DISTRICT},
                'ba ƒë√¨nh': {'normalized': 'Ba ƒê√¨nh', 'city': 'H√† N·ªôi', 'type': LocationType.DISTRICT},
                'ho√†n ki·∫øm': {'normalized': 'Ho√†n Ki·∫øm', 'city': 'H√† N·ªôi', 'type': LocationType.DISTRICT},
                'h·ªìng b√†ng': {'normalized': 'H·ªìng B√†ng', 'city': 'H·∫£i Ph√≤ng', 'type': LocationType.DISTRICT}
            },
            'regions': {
                'mi·ªÅn b·∫Øc': {'normalized': 'Mi·ªÅn B·∫Øc', 'type': LocationType.REGION},
                'mi·ªÅn trung': {'normalized': 'Mi·ªÅn Trung', 'type': LocationType.REGION},
                'mi·ªÅn nam': {'normalized': 'Mi·ªÅn Nam', 'type': LocationType.REGION}
            }
        }
    
    def _load_service_patterns(self) -> Dict[str, List[str]]:
        """Load service requirement patterns"""
        return {
            'kids_friendly': [
                r'(tr·∫ª em|tr·∫ª con|b√©|children|kids)',
                r'(khu vui ch∆°i|playground|ch·ªó.*ch∆°i)',
                r'(gia ƒë√¨nh.*tr·∫ª|family.*kids)'
            ],
            'romantic': [
                r'(l√£ng m·∫°n|romantic|c·∫∑p ƒë√¥i|couple)',
                r'(h·∫πn h√≤|date|t√¨nh y√™u)',
                r'(kh√¥ng gian.*ri√™ng t∆∞|private)'
            ],
            'group_dining': [
                r'(nh√≥m|group|team|c√¥ng ty)',
                r'(t·∫≠p th·ªÉ|ƒë√¥ng ng∆∞·ªùi|many people)',
                r'(ti·ªác|party|celebration)'
            ],
            'luxury': [
                r'(sang tr·ªçng|luxury|cao c·∫•p|premium)',
                r'(vip|exclusive|ƒë·∫≥ng c·∫•p)',
                r'(high.?end|upscale)'
            ],
            'budget': [
                r'(r·∫ª|cheap|budget|gi√°.*th·∫•p)',
                r'(ti·∫øt ki·ªám|affordable|reasonab)',
                r'(sinh vi√™n|student)'
            ],
            'outdoor': [
                r'(ngo√†i tr·ªùi|outdoor|s√¢n v∆∞·ªùn)',
                r'(kh√¥ng gian.*m·ªü|open.?space)',
                r'(view.*ƒë·∫πp|scenic)'
            ],
            'indoor': [
                r'(trong nh√†|indoor|c√≥.*m√°y l·∫°nh)',
                r'(ƒëi·ªÅu h√≤a|air.?con)',
                r'(k√≠n.*gi√≥|enclosed)'
            ]
        }
    
    def _load_target_patterns(self) -> Dict[str, List[str]]:
        """Load target audience patterns"""
        return {
            'family': [
                r'(gia ƒë√¨nh|family|c·∫£ nh√†)',
                r'(b·ªë m·∫π.*con|parents.*children)',
                r'(nhi·ªÅu th·∫ø h·ªá|multi.?generation)'
            ],
            'couple': [
                r'(c·∫∑p ƒë√¥i|couple|hai ng∆∞·ªùi)',
                r'(b·∫°n trai.*b·∫°n g√°i|boyfriend.*girlfriend)',
                r'(ch·ªìng.*v·ª£|husband.*wife)'
            ],
            'friends': [
                r'(b·∫°n b√®|friends|h·ªôi b·∫°n)',
                r'(nh√≥m.*b·∫°n|group.*friends)',
                r'(gathering|get.?together)'
            ],
            'business': [
                r'(c√¥ng vi·ªác|business|meeting)',
                r'(h·ªçp|conference|kh√°ch h√†ng)',
                r'(ƒë·ªëi t√°c|partner|client)'
            ],
            'solo': [
                r'(m·ªôt m√¨nh|solo|individual)',
                r'(c√° nh√¢n|personal|alone)',
                r'(t·ª±.*m·ªôt|by.*myself)'
            ]
        }
    
    def _load_time_patterns(self) -> Dict[str, List[str]]:
        """Load time-related patterns"""
        return {
            'weekend': [
                r'(cu·ªëi tu·∫ßn|weekend|th·ª©.*7|ch·ªß nh·∫≠t)',
                r'(saturday|sunday|ngh·ªâ.*tu·∫ßn)'
            ],
            'weekday': [
                r'(trong tu·∫ßn|weekday|th·ª©.*[2-6])',
                r'(monday|tuesday|wednesday|thursday|friday)',
                r'(ng√†y.*th∆∞·ªùng|working.*day)'
            ],
            'evening': [
                r'(t·ªëi|evening|night|bu·ªïi.*t·ªëi)',
                r'(dinner|b·ªØa.*t·ªëi|ƒÉn.*t·ªëi)'
            ],
            'lunch': [
                r'(tr∆∞a|lunch|bu·ªïi.*tr∆∞a)',
                r'(b·ªØa.*tr∆∞a|ƒÉn.*tr∆∞a|noon)'
            ],
            'morning': [
                r'(s√°ng|morning|bu·ªïi.*s√°ng)',
                r'(breakfast|b·ªØa.*s√°ng|ƒÉn.*s√°ng)'
            ],
            'holiday': [
                r'(l·ªÖ|holiday|ngh·ªâ.*l·ªÖ|festival)',
                r'(t·∫øt|new.*year|christmas|celebration)'
            ]
        }
    
    def _load_modifier_patterns(self) -> Dict[str, List[str]]:
        """Load query modifiers (urgent, flexible, etc.)"""
        return {
            'urgent': [
                r'(g·∫•p|urgent|ngay.*b√¢y.*gi·ªù|immediately)',
                r'(kh·∫©n.*c·∫•p|asap|rush)'
            ],
            'flexible': [
                r'(linh ho·∫°t|flexible|t√πy.*√Ω)',
                r'(kh√¥ng.*c·ªë ƒë·ªãnh|not.*fixed|open)'
            ],
            'specific': [
                r'(c·ª• th·ªÉ|specific|ch√≠nh x√°c|exact)',
                r'(ƒë√∫ng.*nh∆∞|exactly.*like)'
            ],
            'recommendation': [
                r'(ƒë·ªÅ xu·∫•t|recommend|g·ª£i √Ω|suggest)',
                r'(t∆∞ v·∫•n|advice|n√™n.*ch·ªçn)'
            ]
        }
    
    def parse_query(self, query: str) -> QueryComponents:
        """
        Main method ƒë·ªÉ parse user query
        """
        query_lower = query.lower().strip()
        
        # Initialize components
        components = QueryComponents(
            original_query=query,
            intent=QueryIntent.GENERAL_SEARCH,
            service_requirements=[],
            time_requirements=[],
            keywords=[],
            modifiers=[]
        )
        
        # Extract intent
        components.intent, intent_confidence = self._extract_intent(query_lower)
        
        # Extract location
        components.location, components.location_type, location_confidence = self._extract_location(query_lower)
        
        # Extract service requirements
        components.service_requirements, service_confidence = self._extract_service_requirements(query_lower)
        
        # Extract target audience
        components.target_audience, target_confidence = self._extract_target_audience(query_lower)
        
        # Extract time requirements
        components.time_requirements = self._extract_time_requirements(query_lower)
        
        # Extract modifiers
        components.modifiers = self._extract_modifiers(query_lower)
        
        # Extract keywords
        components.keywords = self._extract_keywords(query_lower)
        
        # Calculate overall confidence
        components.confidence = self._calculate_confidence(
            intent_confidence, location_confidence, service_confidence, target_confidence
        )
        
        logger.info(f"üîç Parsed query: {query}")
        logger.info(f"   Intent: {components.intent.value} (confidence: {components.confidence:.2f})")
        logger.info(f"   Location: {components.location}")
        logger.info(f"   Requirements: {components.service_requirements}")
        
        return components
    
    def _extract_intent(self, query: str) -> Tuple[QueryIntent, float]:
        """Extract main intent from query"""
        intent_scores = {}
        
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                score += len(matches)
            
            if score > 0:
                intent_scores[intent] = score
        
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1])
            confidence = min(best_intent[1] / len(query.split()), 1.0)
            return best_intent[0], confidence
        
        return QueryIntent.GENERAL_SEARCH, 0.5
    
    def _extract_location(self, query: str) -> Tuple[Optional[str], Optional[LocationType], float]:
        """Extract location with type detection"""
        # Check cities first
        for location_key, location_info in self.location_patterns['cities'].items():
            if location_key in query:
                return location_info['normalized'], location_info['type'], 0.9
        
        # Check districts
        for district_key, district_info in self.location_patterns['districts'].items():
            if district_key in query:
                return district_info['normalized'], district_info['type'], 0.8
        
        # Check regions
        for region_key, region_info in self.location_patterns['regions'].items():
            if region_key in query:
                return region_info['normalized'], region_info['type'], 0.7
        
        # Use location prepositions to find potential locations
        location_indicators = [r't·∫°i\s+([^,\s]+)', r'·ªü\s+([^,\s]+)', r'trong\s+([^,\s]+)']
        for pattern in location_indicators:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                # Try to normalize the found location
                potential_location = matches[0].strip()
                normalized = self._normalize_location(potential_location)
                if normalized:
                    return normalized, LocationType.CITY, 0.6
        
        return None, None, 0.0
    
    def _normalize_location(self, location: str) -> Optional[str]:
        """Normalize detected location"""
        location_lower = location.lower()
        
        # Check if it matches any known location
        all_locations = {**self.location_patterns['cities'], 
                        **self.location_patterns['districts']}
        
        for key, info in all_locations.items():
            if key in location_lower or location_lower in key:
                return info['normalized']
        
        return None
    
    def _extract_service_requirements(self, query: str) -> Tuple[List[str], float]:
        """Extract service requirements"""
        requirements = []
        total_matches = 0
        
        for requirement, patterns in self.service_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    requirements.append(requirement)
                    total_matches += 1
                    break
        
        confidence = min(total_matches / 3, 1.0) if requirements else 0.0
        return requirements, confidence
    
    def _extract_target_audience(self, query: str) -> Tuple[Optional[str], float]:
        """Extract target audience"""
        audience_scores = {}
        
        for audience, patterns in self.target_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    score += 1
            
            if score > 0:
                audience_scores[audience] = score
        
        if audience_scores:
            best_audience = max(audience_scores.items(), key=lambda x: x[1])
            confidence = min(best_audience[1] / 2, 1.0)
            return best_audience[0], confidence
        
        return None, 0.0
    
    def _extract_time_requirements(self, query: str) -> List[str]:
        """Extract time-related requirements"""
        time_reqs = []
        
        for time_type, patterns in self.time_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    time_reqs.append(time_type)
                    break
        
        return time_reqs
    
    def _extract_modifiers(self, query: str) -> List[str]:
        """Extract query modifiers"""
        modifiers = []
        
        for modifier, patterns in self.modifier_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    modifiers.append(modifier)
                    break
        
        return modifiers
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Extract important keywords for search"""
        # Remove common Vietnamese stop words
        stop_words = {
            't√¥i', 't·∫°i', '·ªü', 'trong', 'c√≥', 'l√†', 'v√†', 'v·ªõi', 'cho', 'c·ªßa', 
            'm·ªôt', 'c√°c', 'n√†y', 'ƒë√≥', 'ƒë∆∞·ª£c', 's·∫Ω', 'ƒë√£', 't·ª´', 'v·ªÅ', 'nh∆∞',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'
        }
        
        # Split and clean words
        words = re.findall(r'\b\w+\b', query.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        return keywords[:10]  # Limit to top 10 keywords
    
    def _calculate_confidence(self, intent_conf: float, location_conf: float, 
                            service_conf: float, target_conf: float) -> float:
        """Calculate overall parsing confidence"""
        # Weighted average of individual confidences
        weights = [0.3, 0.3, 0.2, 0.2]  # intent, location, service, target
        confidences = [intent_conf, location_conf, service_conf, target_conf]
        
        weighted_sum = sum(w * c for w, c in zip(weights, confidences))
        return min(weighted_sum, 1.0)
    
    def generate_search_strategy(self, components: QueryComponents) -> Dict[str, Any]:
        """
        Generate search strategy based on parsed components
        """
        strategy = {
            'primary_field': 'combined_embedding',
            'boost_factors': {},
            'filters': {},
            'ranking_weights': {
                'semantic_similarity': 0.4,
                'location_match': 0.3,
                'service_match': 0.2,
                'target_match': 0.1
            }
        }
        
        # Adjust strategy based on intent and components
        if components.location and components.location_type == LocationType.CITY:
            strategy['primary_field'] = 'location_embedding'
            strategy['boost_factors']['location_exact_match'] = 2.0
            strategy['filters']['location'] = components.location
            strategy['ranking_weights']['location_match'] = 0.5
            strategy['ranking_weights']['semantic_similarity'] = 0.3
        
        if 'kids_friendly' in components.service_requirements:
            strategy['boost_factors']['kids_area'] = 1.5
            strategy['filters']['has_kids_area'] = True
        
        if 'luxury' in components.service_requirements:
            strategy['filters']['price_range'] = ['Premium', 'Luxury']
        elif 'budget' in components.service_requirements:
            strategy['filters']['price_range'] = ['Budget', 'Mid-range']
        
        # Time-based adjustments
        if 'weekend' in components.time_requirements:
            strategy['boost_factors']['weekend_available'] = 1.2
        
        # Confidence-based adjustments
        if components.confidence < 0.5:
            strategy['ranking_weights']['semantic_similarity'] = 0.6  # Fall back to semantic
            strategy['primary_field'] = 'combined_embedding'
        
        return strategy
    
    def explain_parsing(self, components: QueryComponents) -> str:
        """
        Generate human-readable explanation of parsing results
        """
        explanation = f"Ph√¢n t√≠ch query: '{components.original_query}'\n"
        explanation += f"- √ù ƒë·ªãnh: {components.intent.value}\n"
        
        if components.location:
            explanation += f"- ƒê·ªãa ƒëi·ªÉm: {components.location} ({components.location_type.value})\n"
        
        if components.service_requirements:
            explanation += f"- Y√™u c·∫ßu d·ªãch v·ª•: {', '.join(components.service_requirements)}\n"
        
        if components.target_audience:
            explanation += f"- ƒê·ªëi t∆∞·ª£ng: {components.target_audience}\n"
        
        if components.time_requirements:
            explanation += f"- Th·ªùi gian: {', '.join(components.time_requirements)}\n"
        
        explanation += f"- ƒê·ªô tin c·∫≠y: {components.confidence:.2f}"
        
        return explanation
