#!/usr/bin/env python3
"""
Vector h√≥a d·ªØ li·ªáu voucher t·ª´ file Excel m·ªõi
Processing importvoucher.xlsx v√† importvoucher2.xlsx
"""

import sys
import os
import pandas as pd
import json
from datetime import datetime
import asyncio
import logging

# Add backend directory to path
sys.path.append('/Users/1-tiennv-m/1MG/Projects/LLM/voucher_assistant/backend')

from vector_store import VectorStore

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VoucherDataProcessor:
    """X·ª≠ l√Ω v√† vector h√≥a d·ªØ li·ªáu voucher t·ª´ Excel files"""
    
    def __init__(self):
        self.vector_store = VectorStore()
        self.processed_count = 0
        self.error_count = 0
        
    def clean_text(self, text):
        """Clean v√† chu·∫©n h√≥a text"""
        if pd.isna(text):
            return ""
        return str(text).strip()
    
    def read_excel_file(self, file_path):
        """ƒê·ªçc file Excel v√† tr·∫£ v·ªÅ DataFrame"""
        try:
            logger.info(f"üìñ ƒê·ªçc file: {file_path}")
            
            # Try different sheet names and indexes
            try:
                df = pd.read_excel(file_path, sheet_name=0)
            except Exception as e:
                logger.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c sheet ƒë·∫ßu ti√™n: {e}")
                # Try reading all sheets
                excel_file = pd.ExcelFile(file_path)
                logger.info(f"Available sheets: {excel_file.sheet_names}")
                df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])
            
            logger.info(f"‚úÖ ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ file")
            logger.info(f"Columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc file {file_path}: {e}")
            return None
    
    def extract_voucher_info(self, row, row_index, source_file):
        """Extract th√¥ng tin voucher t·ª´ m·ªói row"""
        try:
            # Try to map common column names
            voucher_data = {}
            
            # Common mappings
            column_mappings = {
                'name': ['Name', 'name', 'T√™n', 'Voucher Name', 'VoucherName', 'title', 'Title'],
                'description': ['Desc', 'Description', 'M√¥ t·∫£', 'desc', 'description'],
                'usage': ['Usage', 'usage', 'S·ª≠ d·ª•ng', 'C√°ch s·ª≠ d·ª•ng'],
                'terms': ['TermOfUse', 'Terms', 'ƒêi·ªÅu kho·∫£n', 'terms', 'term_of_use'],
                'price': ['Price', 'price', 'Gi√°', 'Gi√° tr·ªã'],
                'unit': ['Unit', 'unit', 'ƒê∆°n v·ªã'],
                'merchant': ['Merchant', 'merchant', 'ƒê·ªëi t√°c', 'Merrchant']
            }
            
            # Extract data based on available columns
            for key, possible_cols in column_mappings.items():
                value = ""
                for col in possible_cols:
                    if col in row.index and pd.notna(row[col]):
                        value = str(row[col]).strip()
                        break
                voucher_data[key] = value
            
            # If no specific mapping found, use all columns
            if not any(voucher_data.values()):
                all_text = []
                for col, val in row.items():
                    if pd.notna(val):
                        all_text.append(f"{col}: {val}")
                voucher_data['content'] = " | ".join(all_text)
            else:
                # Combine all fields into content
                content_parts = []
                for key, value in voucher_data.items():
                    if value and value.strip():
                        content_parts.append(f"{key.capitalize()}: {value}")
                voucher_data['content'] = " | ".join(content_parts)
            
            # Generate voucher ID
            name = voucher_data.get('name', f'voucher_{row_index}')
            voucher_data['voucher_id'] = f"voucher_{hash(name)}_{row_index}"
            voucher_data['voucher_name'] = name or f"Voucher {row_index}"
            voucher_data['source'] = source_file
            voucher_data['row_index'] = row_index
            
            return voucher_data
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract voucher t·ª´ row {row_index}: {e}")
            return None
    
    async def process_excel_file(self, file_path):
        """X·ª≠ l√Ω m·ªôt file Excel"""
        try:
            # Read Excel file
            df = self.read_excel_file(file_path)
            if df is None:
                return False
            
            source_file = os.path.basename(file_path)
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(df)} vouchers t·ª´ {source_file}")
            
            # Process each row
            for index, row in df.iterrows():
                try:
                    # Extract voucher info
                    voucher_data = self.extract_voucher_info(row, index, source_file)
                    if not voucher_data:
                        continue
                    
                    # Vector h√≥a v√† l∆∞u v√†o Elasticsearch
                    await self.store_voucher_vector(voucher_data)
                    self.processed_count += 1
                    
                    if self.processed_count % 5 == 0:
                        logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {self.processed_count} vouchers...")
                        
                except Exception as e:
                    logger.error(f"‚ùå L·ªói x·ª≠ l√Ω row {index}: {e}")
                    self.error_count += 1
            
            logger.info(f"‚úÖ Ho√†n th√†nh x·ª≠ l√Ω file {source_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω file {file_path}: {e}")
            return False
    
    async def store_voucher_vector(self, voucher_data):
        """Vector h√≥a v√† l∆∞u voucher v√†o Elasticsearch"""
        try:
            content = voucher_data.get('content', '')
            if not content:
                logger.warning(f"‚ö†Ô∏è Voucher {voucher_data['voucher_id']} kh√¥ng c√≥ content")
                return
            
            # T·∫°o embedding vector
            embedding = self.vector_store.create_embedding(content)
            
            # Prepare document for Elasticsearch
            doc = {
                'voucher_id': voucher_data['voucher_id'],
                'voucher_name': voucher_data['voucher_name'],
                'content': content,
                'embedding': embedding,
                'metadata': {
                    'source': voucher_data['source'],
                    'row_index': voucher_data['row_index'],
                    'merchant': voucher_data.get('merchant', ''),
                    'price': voucher_data.get('price', ''),
                    'total_columns': len(voucher_data)
                },
                'created_at': datetime.now().isoformat()
            }
            
            # Store in Elasticsearch
            if self.vector_store.es:
                try:
                    result = self.vector_store.es.index(
                        index=self.vector_store.index_name,
                        id=voucher_data['voucher_id'],
                        body=doc
                    )
                    logger.debug(f"üíæ Stored voucher {voucher_data['voucher_id']}: {result['result']}")
                except Exception as e:
                    logger.error(f"‚ùå L·ªói l∆∞u Elasticsearch: {e}")
            else:
                logger.warning("‚ö†Ô∏è Elasticsearch kh√¥ng available")
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói store voucher vector: {e}")
    
    async def process_all_files(self):
        """X·ª≠ l√Ω t·∫•t c·∫£ file Excel"""
        data_dir = "/Users/1-tiennv-m/1MG/Projects/LLM/data"
        excel_files = [
            "importvoucher.xlsx",
            "importvoucher2.xlsx"
        ]
        
        logger.info("üöÄ B·∫Øt ƒë·∫ßu vector h√≥a d·ªØ li·ªáu voucher")
        logger.info("=" * 50)
        
        # Ensure vector store is ready
        if not self.vector_store.is_ready:
            logger.error("‚ùå Vector Store ch∆∞a s·∫µn s√†ng")
            return
        
        # Process each file
        for excel_file in excel_files:
            file_path = os.path.join(data_dir, excel_file)
            
            if os.path.exists(file_path):
                logger.info(f"\nüìÅ X·ª≠ l√Ω file: {excel_file}")
                await self.process_excel_file(file_path)
            else:
                logger.warning(f"‚ö†Ô∏è File kh√¥ng t·ªìn t·∫°i: {file_path}")
        
        # Summary
        logger.info("\n" + "=" * 50)
        logger.info("üìä K·∫æT QU·∫¢ VECTOR H√ìA")
        logger.info("=" * 50)
        logger.info(f"‚úÖ T·ªïng s·ªë vouchers ƒë√£ x·ª≠ l√Ω: {self.processed_count}")
        logger.info(f"‚ùå S·ªë l·ªói: {self.error_count}")
        
        # Check final status
        if self.vector_store.es:
            try:
                count_response = self.vector_store.es.count(index=self.vector_store.index_name)
                total_docs = count_response.get('count', 0)
                logger.info(f"üìà T·ªïng documents trong Elasticsearch: {total_docs}")
            except Exception as e:
                logger.error(f"‚ùå L·ªói ki·ªÉm tra s·ªë l∆∞·ª£ng docs: {e}")
        
        logger.info("üéâ Ho√†n th√†nh vector h√≥a d·ªØ li·ªáu!")

async def main():
    """Main function"""
    processor = VoucherDataProcessor()
    await processor.process_all_files()

if __name__ == "__main__":
    asyncio.run(main())
