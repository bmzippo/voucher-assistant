#!/usr/bin/env python3
"""
OneU AI Voucher Assistant - Knowledge Base Setup
Thi·∫øt l·∫≠p knowledge base t·ª´ d·ªØ li·ªáu voucher Excel
"""

import sys
import os

# Ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies
def check_and_install_dependencies():
    """Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"""
    required_packages = [
        'pandas',
        'numpy', 
        'elasticsearch>=8.0.0,<9.0.0',  # Fix version compatibility
        'sentence-transformers',
        'openpyxl',
        'python-dotenv',
        'requests'
    ]
    
    missing_packages = []
    
    # Ki·ªÉm tra t·ª´ng package
    for package in required_packages:
        package_name = package.split('>=')[0].split('==')[0].replace('-', '_')
        try:
            __import__(package_name)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"‚ùå Thi·∫øu c√°c th∆∞ vi·ªán: {', '.join(missing_packages)}")
        print("üîß ƒêang c√†i ƒë·∫∑t dependencies...")
        
        import subprocess
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                *missing_packages
            ])
            print("‚úÖ C√†i ƒë·∫∑t th√†nh c√¥ng!")
        except subprocess.CalledProcessError:
            print("‚ùå L·ªói c√†i ƒë·∫∑t. Vui l√≤ng ch·∫°y:")
            print(f"pip install {' '.join(missing_packages)}")
            sys.exit(1)

# Ki·ªÉm tra dependencies tr∆∞·ªõc khi import
check_and_install_dependencies()

try:
    import pandas as pd
    import numpy as np
    from elasticsearch import Elasticsearch
    from sentence_transformers import SentenceTransformer
    import json
    import logging
    from dotenv import load_dotenv
    import time
    import requests
    import warnings
    
    # Suppress Elasticsearch warnings
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    
except ImportError as e:
    print(f"‚ùå L·ªói import: {e}")
    print("üîß Vui l√≤ng ch·∫°y: pip install -r requirements.txt")
    sys.exit(1)

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class VoucherKnowledgeBaseSetup:
    def __init__(self):
        self.es_host = os.getenv('ELASTICSEARCH_HOST', 'localhost:9200')
        self.es_index = os.getenv('ELASTICSEARCH_INDEX', 'voucher_knowledge_base')
        self.model_name = os.getenv('EMBEDDING_MODEL', 'dangvantuan/vietnamese-embedding')
        self.embedding_dimension = int(os.getenv('EMBEDDING_DIMENSION', '768'))  # Default to 768 for Vietnamese model
        
        # Initialize components
        self.es = None
        self.model = None
        self.actual_embedding_dimension = None
        
    def wait_for_elasticsearch(self, max_retries=30):
        """ƒê·ª£i Elasticsearch kh·ªüi ƒë·ªông"""
        logger.info("üîç ƒêang ki·ªÉm tra Elasticsearch...")
        
        for i in range(max_retries):
            try:
                response = requests.get(f"http://{self.es_host}/_cluster/health", timeout=5)
                if response.status_code == 200:
                    logger.info("‚úÖ Elasticsearch ƒë√£ s·∫µn s√†ng!")
                    return True
            except requests.RequestException:
                pass
            
            logger.info(f"‚è≥ ƒêang ƒë·ª£i Elasticsearch... ({i+1}/{max_retries})")
            time.sleep(2)
        
        logger.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Elasticsearch")
        return False
    
    def setup_elasticsearch(self):
        """Thi·∫øt l·∫≠p k·∫øt n·ªëi Elasticsearch v·ªõi version compatibility"""
        if not self.wait_for_elasticsearch():
            logger.error("‚ùå Elasticsearch kh√¥ng kh·∫£ d·ª•ng")
            return False
            
        try:
            # T·∫°o Elasticsearch client v·ªõi c·∫•u h√¨nh ph√π h·ª£p
            self.es = Elasticsearch(
                [f"http://{self.es_host}"],
                verify_certs=False,
                request_timeout=30,
                retry_on_timeout=True,
                max_retries=3
            )
            
            # Test connection v·ªõi error handling
            try:
                info = self.es.info()
                version = info.get('version', {}).get('number', 'unknown')
                logger.info(f"‚úÖ K·∫øt n·ªëi Elasticsearch th√†nh c√¥ng: v{version}")
                return True
            except Exception as version_error:
                logger.warning(f"‚ö†Ô∏è Version check failed: {version_error}")
                # Try basic ping instead
                if self.es.ping():
                    logger.info("‚úÖ K·∫øt n·ªëi Elasticsearch th√†nh c√¥ng (basic ping)")
                    return True
                else:
                    raise Exception("Ping failed")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói k·∫øt n·ªëi Elasticsearch: {e}")
            logger.info("üîß Th·ª≠ s·ª≠ d·ª•ng phi√™n b·∫£n Elasticsearch client t∆∞∆°ng th√≠ch...")
            
            # Try with legacy client configuration
            try:
                self.es = Elasticsearch(
                    [{'host': 'localhost', 'port': 9200}],
                    verify_certs=False,
                    timeout=30
                )
                
                if self.es.ping():
                    logger.info("‚úÖ K·∫øt n·ªëi Elasticsearch th√†nh c√¥ng (legacy mode)")
                    return True
                    
            except Exception as legacy_error:
                logger.error(f"‚ùå Legacy connection failed: {legacy_error}")
                
            return False
    
    def setup_embedding_model(self):
        """Thi·∫øt l·∫≠p model embedding"""
        try:
            logger.info(f"ü§ñ ƒêang t·∫£i model embedding: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            
            # Test model ƒë·ªÉ l·∫•y actual dimension
            test_embedding = self.model.encode("test")
            self.actual_embedding_dimension = len(test_embedding)
            logger.info(f"‚úÖ Model embedding ƒë√£ s·∫µn s√†ng! Dimension: {self.actual_embedding_dimension}")
            
            # Update embedding dimension n·∫øu kh√°c v·ªõi config
            if self.actual_embedding_dimension != self.embedding_dimension:
                logger.info(f"üîÑ C·∫≠p nh·∫≠t embedding dimension t·ª´ {self.embedding_dimension} th√†nh {self.actual_embedding_dimension}")
                self.embedding_dimension = self.actual_embedding_dimension
            
            return True
        except Exception as e:
            logger.error(f"‚ùå L·ªói t·∫£i model: {e}")
            return False
    
    def create_index_mapping(self):
        """T·∫°o mapping cho Elasticsearch index v·ªõi version compatibility"""
        mapping = {
            "mappings": {
                "properties": {
                    "voucher_id": {"type": "keyword"},
                    "voucher_name": {
                        "type": "text", 
                        "analyzer": "standard",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "content": {
                        "type": "text", 
                        "analyzer": "standard"
                    },
                    "content_type": {"type": "keyword"},
                    "embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dimension,  # Use actual dimension from model
                        "index": True,
                        "similarity": "cosine"
                    },
                    "metadata": {"type": "object"},
                    "created_at": {"type": "date"}
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            }
        }
        
        try:
            # X√≥a index c≈© n·∫øu t·ªìn t·∫°i
            if self.es.indices.exists(index=self.es_index):
                self.es.indices.delete(index=self.es_index)
                logger.info(f"üóëÔ∏è ƒê√£ x√≥a index c≈©: {self.es_index}")
            
            # T·∫°o index m·ªõi
            response = self.es.indices.create(index=self.es_index, body=mapping)
            logger.info(f"‚úÖ ƒê√£ t·∫°o index: {self.es_index} v·ªõi dimension: {self.embedding_dimension}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói t·∫°o index: {e}")
            
            # Try with simpler mapping if advanced features fail
            try:
                simple_mapping = {
                    "mappings": {
                        "properties": {
                            "voucher_id": {"type": "keyword"},
                            "voucher_name": {"type": "text"},
                            "content": {"type": "text"},
                            "content_type": {"type": "keyword"},
                            "embedding": {
                                "type": "dense_vector",
                                "dims": self.embedding_dimension  # Use actual dimension
                            },
                            "metadata": {"type": "object"},
                            "created_at": {"type": "date"}
                        }
                    }
                }
                
                response = self.es.indices.create(index=self.es_index, body=simple_mapping)
                logger.info(f"‚úÖ ƒê√£ t·∫°o index v·ªõi simple mapping: {self.es_index} (dims: {self.embedding_dimension})")
                return True
                
            except Exception as simple_error:
                logger.error(f"‚ùå L·ªói t·∫°o simple index: {simple_error}")
                return False
    
    def load_voucher_data(self):
        """T·∫£i d·ªØ li·ªáu voucher t·ª´ Excel file"""
        # T√¨m file Excel trong c√°c ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
        possible_paths = [
            "../data/temp voucher.xlsx",
            "./data/temp voucher.xlsx", 
            "../temp voucher.xlsx",
            "./temp voucher.xlsx",
            "temp voucher.xlsx"
        ]
        
        excel_file = None
        for path in possible_paths:
            if os.path.exists(path):
                excel_file = path
                break
        
        if not excel_file:
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file Excel trong c√°c ƒë∆∞·ªùng d·∫´n: {possible_paths}")
            logger.info("üí° Vui l√≤ng ƒë·∫∑t file 'temp voucher.xlsx' trong th∆∞ m·ª•c data/")
            return None
        
        try:
            df = pd.read_excel(excel_file)
            logger.info(f"‚úÖ ƒê√£ t·∫£i {len(df)} vouchers t·ª´ {excel_file}")
            
            # In th√¥ng tin c·ªôt ƒë·ªÉ debug
            logger.info(f"üìã C√°c c·ªôt trong Excel: {list(df.columns)}")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc Excel: {e}")
            return None
    
    def process_and_index_vouchers(self, df):
        """X·ª≠ l√Ω v√† index vouchers v√†o Elasticsearch"""
        logger.info("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v√† index vouchers...")
        
        total_docs = 0
        
        for idx, row in df.iterrows():
            try:
                voucher_id = f"voucher_{idx}"
                
                # L·∫•y t√™n voucher t·ª´ c√°c c·ªôt c√≥ th·ªÉ
                voucher_name = None
                for col_name in ['T√™n voucher', 'Voucher Name', 'Name', 'Title']:
                    if col_name in df.columns and pd.notna(row.get(col_name)):
                        voucher_name = str(row[col_name])
                        break
                
                if not voucher_name:
                    voucher_name = f'Voucher {idx + 1}'
                
                # T·∫°o n·ªôi dung t·ª´ c√°c c·ªôt
                content_parts = []
                for col in df.columns:
                    if pd.notna(row[col]) and str(row[col]).strip():
                        content_parts.append(f"{col}: {str(row[col]).strip()}")
                
                content = " | ".join(content_parts)
                
                # ƒê·∫£m b·∫£o content kh√¥ng r·ªóng
                if not content.strip():
                    logger.warning(f"‚ö†Ô∏è Voucher {idx} c√≥ n·ªôi dung r·ªóng, b·ªè qua")
                    continue
                
                # T·∫°o embedding
                try:
                    embedding = self.model.encode(content).tolist()
                except Exception as embed_error:
                    logger.error(f"‚ùå L·ªói t·∫°o embedding cho voucher {idx}: {embed_error}")
                    continue
                
                # T·∫°o document
                doc = {
                    "voucher_id": voucher_id,
                    "voucher_name": voucher_name,
                    "content": content,
                    "content_type": "voucher_details",
                    "embedding": embedding,
                    "metadata": {
                        "source": "temp_voucher.xlsx",
                        "row_index": idx,
                        "total_columns": len(df.columns)
                    },
                    "created_at": pd.Timestamp.now().isoformat()
                }
                
                # Index document
                self.es.index(index=self.es_index, id=voucher_id, document=doc)
                total_docs += 1
                
                if (idx + 1) % 5 == 0 or idx + 1 == len(df):
                    logger.info(f"üìù ƒê√£ x·ª≠ l√Ω {idx + 1}/{len(df)} vouchers")
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω voucher {idx}: {e}")
                continue
        
        logger.info(f"‚úÖ Ho√†n th√†nh! ƒê√£ index {total_docs} documents")
        return total_docs
    
    def verify_setup(self):
        """Ki·ªÉm tra setup"""
        try:
            # Refresh index
            self.es.indices.refresh(index=self.es_index)
            
            # ƒê·∫øm documents
            count_response = self.es.count(index=self.es_index)
            count = count_response['count']
            logger.info(f"üìä T·ªïng s·ªë documents trong index: {count}")
            
            if count == 0:
                logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ documents n√†o trong index")
                return False
            
            # Test search
            test_query = {
                "query": {
                    "match_all": {}
                },
                "size": 1
            }
            
            result = self.es.search(index=self.es_index, body=test_query)
            if result['hits']['total']['value'] > 0:
                logger.info("‚úÖ Test search th√†nh c√¥ng!")
                
                # In th√¥ng tin sample document
                sample_doc = result['hits']['hits'][0]['_source']
                logger.info(f"üìÑ Sample document: {sample_doc['voucher_name']}")
                return True
            else:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ test search")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói verification: {e}")
            return False
    
    def run_setup(self):
        """Ch·∫°y to√†n b·ªô setup process"""
        logger.info("üöÄ B·∫Øt ƒë·∫ßu setup Knowledge Base cho OneU AI Voucher Assistant")
        
        # 1. Setup Elasticsearch
        if not self.setup_elasticsearch():
            return False
        
        # 2. Setup embedding model
        if not self.setup_embedding_model():
            return False
        
        # 3. T·∫°o index mapping
        if not self.create_index_mapping():
            return False
        
        # 4. Load voucher data
        df = self.load_voucher_data()
        if df is None:
            return False
        
        # 5. Process v√† index vouchers
        docs_count = self.process_and_index_vouchers(df)
        if docs_count == 0:
            return False
        
        # 6. Verify setup
        if not self.verify_setup():
            return False
        
        logger.info("üéâ Setup Knowledge Base ho√†n th√†nh th√†nh c√¥ng!")
        return True

def main():
    """Main function"""
    setup = VoucherKnowledgeBaseSetup()
    
    if setup.run_setup():
        print("\nüéâ OneU AI Voucher Assistant Knowledge Base ƒë√£ s·∫µn s√†ng!")
        print(f"üìä Index: {setup.es_index}")
        print(f"üîç Elasticsearch: http://{setup.es_host}")
        print("\nüöÄ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y backend API!")
        print("\nüìù ƒê·ªÉ test knowledge base:")
        print("   curl http://localhost:8000/api/search?q=voucher")
    else:
        print("\n‚ùå Setup th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra logs v√† th·ª≠ l·∫°i.")
        sys.exit(1)

if __name__ == "__main__":
    main()